An N-gram language model scores words based on the preceding window of context. Although the N-gram model is not very sophisticated and fails to handle long-range dependencies and abstract semantic information, we can actually see this as a feature rather than a bug for this task. Other language models, such as those based on Recurrent Neural Networks or Transformers, are better at capturing long-range dependencies and higher levels of abstraction. For plagiarism, however, the emphasis is on copied sequences of words, not on similarities at an abstract level. A paraphrasing should not set off an alarm, but a direct copying should. I found that an N-gram window of 4 worked well, and it also aligns with the advice of many teachers not to use more than three words in a row from a source.